{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from funciones import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"Datos/AneuxSplines/zero-root/tokenized/p15/train\"\n",
    "\n",
    "TRAIN = True\n",
    "WANDB_UPLOAD = True\n",
    "\n",
    "vocab_size = 258        # 256 : EOS token , 257 : pad token\n",
    "max_size = 2256 + 2\n",
    "pad_token = 257\n",
    "eos_token = 256\n",
    "\n",
    "epochs = 50000\n",
    "lr = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "\n",
    "        self.samples = []\n",
    "        self._load_files(folder_path)\n",
    "\n",
    "    def _load_files(self, folder_path):\n",
    "\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        for file_name in files:\n",
    "\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            self.samples.append(torch.load(file_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        eos = torch.tensor([eos_token])\n",
    "        seq = torch.cat((eos, self.samples[idx], eos))\n",
    "\n",
    "        return torch.tensor(seq, dtype = torch.long)\n",
    "\n",
    "def custom_collate(batch, pad_token_id = 257):\n",
    "    return pad_sequence(batch, batch_first = True, padding_value = pad_token_id)\n",
    "\n",
    "def create_attention_mask(batch, pad_token_id):\n",
    "    return (batch != pad_token_id).long()  # 1 for real tokens, 0 for padding\n",
    "\n",
    "def create_gpt2_model(vocab_size, max_size, pad_token):\n",
    "    \n",
    "    config = GPT2Config(\n",
    "\n",
    "        vocab_size = vocab_size,\n",
    "        n_embd = 512,  # Size of embeddings\n",
    "        n_layer = 6,   # Number of layers\n",
    "        n_head = 8,    # Number of attention heads\n",
    "        n_positions = max_size,  # Increase max sequence length\n",
    "        n_ctx = max_size, \n",
    "        pad_token_id = pad_token\n",
    "    )\n",
    "\n",
    "    return GPT2LMHeadModel(config)\n",
    "\n",
    "dataset = TokenDataset(dataset_name)\n",
    "dataloader = DataLoader(dataset, batch_size = 4, collate_fn = custom_collate, shuffle = False)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest sequence length : 2258\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for seq in dataset: \n",
    "    if len(seq) > max: max = len(seq)\n",
    "\n",
    "print(\"Largest sequence length :\", max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     36\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m---> 38\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_losses = []\n",
    "errors = []\n",
    "\n",
    "if TRAIN:\n",
    "\n",
    "    if WANDB_UPLOAD:\n",
    "\n",
    "        wandb.login(key = \"2511bccb1c20c8149e91d2ff7ad5b57fab7df870\")\n",
    "        wandb.init(project = \"gpt2\", entity = \"vesselgpt\")\n",
    "\n",
    "        wandb.config.update({\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"dataset_size\": len(dataset),\n",
    "            \"vocab_size\": vocab_size\n",
    "        })\n",
    "\n",
    "    model = create_gpt2_model(vocab_size, max_size, pad_token)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * epochs)\n",
    "    best_loss = float('inf') \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        total_loss = 0\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            attention_mask = create_attention_mask(batch, pad_token).to(device)  \n",
    "            outputs = model(batch, labels = batch, attention_mask = attention_mask)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            del outputs, loss, batch\n",
    "            gc.collect()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {avg_loss}\")\n",
    "        if WANDB_UPLOAD: wandb.log({\"epoch\": epoch, \"avg_loss\": avg_loss})\n",
    "\n",
    "        # save best model\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        best_loss = save_best_model_gpt2(model, optimizer, epoch, avg_loss, best_loss, \"models/gpt2/aneux_splines_zero_root\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
